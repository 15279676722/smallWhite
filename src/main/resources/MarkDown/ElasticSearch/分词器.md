## 1. 概述

分词器是Elasticsearch中很重要的一个组件，用来将一段文本分析成一个一个的词，Elasticsearch再根据这些词去做倒排索引。

今天我们就来聊聊分词器的相关知识。

## 2. 内置分词器

### 2.1 概述

Elasticsearch 中内置了一些分词器，这些分词器只能对英文进行分词处理，无法将中文的词识别出来。

### 2.2 内置分词器介绍

standard：标准分词器，是Elasticsearch中默认的分词器，可以拆分英文单词，大写字母统一转换成小写。

simple：按非字母的字符分词，例如：数字、标点符号、特殊字符等，会去掉非字母的词，大写字母统一转换成小写。

whitespace：简单按照空格进行分词，相当于按照空格split了一下，大写字母不会转换成小写。

stop：会去掉无意义的词，例如：the、a、an 等，大写字母统一转换成小写。

keyword：不拆分，整个文本当作一个词。

### 2.3 查看分词效果通用接口

```json
get _analyze
{
    "analyzer": "standard", 
    "text": "我是一名Java高级程序员"
}
```

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "一",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    },
    {
      "token" : "名",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "java",
      "start_offset" : 4,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "高",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<IDEOGRAPHIC>",
      "position" : 5
    },
    {
      "token" : "级",
      "start_offset" : 9,
      "end_offset" : 10,
      "type" : "<IDEOGRAPHIC>",
      "position" : 6
    },
    {
      "token" : "程",
      "start_offset" : 10,
      "end_offset" : 11,
      "type" : "<IDEOGRAPHIC>",
      "position" : 7
    },
    {
      "token" : "序",
      "start_offset" : 11,
      "end_offset" : 12,
      "type" : "<IDEOGRAPHIC>",
      "position" : 8
    },
    {
      "token" : "员",
      "start_offset" : 12,
      "end_offset" : 13,
      "type" : "<IDEOGRAPHIC>",
      "position" : 9
    }
  ]
}

```

## 3. IK分词器

### 3.1 概述

Elasticsearch中内置的分词器不能对中文进行分词，因此我们需要再安装一个能够支持中文的分词器，IK分词器就是个不错的选择。

### 3.2 下载IK分词器

下载网址

https://github.com/medcl/elasticsearch-analysis-ik/tags

找到对应的版本进行下载

### 3.3 IK分词器的安装

1）把 .zip 文件进行解压缩

2）将IK分词器文件夹拷贝到 docker容器 es 的 plugins目录下  docker cp /Users/yangqiang/data/es/plugins/elasticsearch-analysis-ik-7.9.2  e117168f6bf9:/usr/share/elasticsearch/plugins

3）重启Elasticsearch

### 3.4 IK分词器介绍

ik_max_word: 会将文本做最细粒度的拆分，适合 Term Query；

ik_smart: 会做最粗粒度的拆分，适合 Phrase 查询。

**IK分词器介绍来源于GitHub：[https://github.com/medcl/elasticsearch-analysis-ik](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fmedcl%2Felasticsearch-analysis-ik)**

## 3.5 分词效果

```json
GET /_analyze
{
    "analyzer": "ik_max_word", 
    "text": "我是一名Java高级程序员"
}
```

```json
{
    "tokens": [
        {
            "token": "我",
            "start_offset": 0,
            "end_offset": 1,
            "type": "CN_CHAR",
            "position": 0
        },
        {
            "token": "是",
            "start_offset": 1,
            "end_offset": 2,
            "type": "CN_CHAR",
            "position": 1
        },
        {
            "token": "一名",
            "start_offset": 2,
            "end_offset": 4,
            "type": "CN_WORD",
            "position": 2
        },
        {
            "token": "一",
            "start_offset": 2,
            "end_offset": 3,
            "type": "TYPE_CNUM",
            "position": 3
        },
        {
            "token": "名",
            "start_offset": 3,
            "end_offset": 4,
            "type": "COUNT",
            "position": 4
        },
        {
            "token": "java",
            "start_offset": 4,
            "end_offset": 8,
            "type": "ENGLISH",
            "position": 5
        },
        {
            "token": "高级",
            "start_offset": 8,
            "end_offset": 10,
            "type": "CN_WORD",
            "position": 6
        },
        {
            "token": "程序员",
            "start_offset": 10,
            "end_offset": 13,
            "type": "CN_WORD",
            "position": 7
        },
        {
            "token": "程序",
            "start_offset": 10,
            "end_offset": 12,
            "type": "CN_WORD",
            "position": 8
        },
        {
            "token": "员",
            "start_offset": 12,
            "end_offset": 13,
            "type": "CN_CHAR",
            "position": 9
        }
    ]
}
```

## 4. 自定义词库

### 4.1 概述

在进行中文分词时，经常出现分析出的词不是我们想要的，这时我们就需要在IK分词器中自定义我们自己词库。

例如：追风人，分词后，只有 追风 和 人，而没有 追风人，导致倒排索引后查询时，用户搜 追风 或 人 可以搜到 追风人，搜 追风人 反而搜不到 追风人。

### 4.2 自定义词库

\# cd /usr/local/elasticsearch-7.14.1/plugins/ik/config

\# vi IKAnalyzer.cfg.xml

在配置文件中增加自己的字典

![img](https:////upload-images.jianshu.io/upload_images/27336903-4675b2fdfb5b63e8.png?imageMogr2/auto-orient/strip|imageView2/2/w/758/format/webp)

image

\# vi my.dic

在文本中加入 奥利给 保存。

重启Elasticsearch即可。

### 5.修改索引的分词器

> 1.关闭索引
>
> POST article/_close
>
> 2.修改为ik_max_word策略
>
> PUT article/_settings
> {
>     "analysis": {
>         "analyzer": {
>             "my_token_filter": {        
>                 "type": "ik_max_word",
>                 "stopwords": "_english_"
>             }
>         }
>     }
> }
>
> 3.打开索引
>
> POST article/_open